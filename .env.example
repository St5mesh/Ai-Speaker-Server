# GPU Stack Environment Variables
# Copy this file to .env and fill in your values
#
# Created: December 13, 2025
# Version: 1.0.0-prototype

# ============================================================================
# CUDA/GPU Configuration
# ============================================================================

# Set visible GPU devices (0-indexed). Use all available by default.
# Examples:
#   CUDA_VISIBLE_DEVICES=0        # Use GPU 0 only
#   CUDA_VISIBLE_DEVICES=0,1      # Use GPU 0 and 1
#   CUDA_VISIBLE_DEVICES=""       # Use CPU only
CUDA_VISIBLE_DEVICES=0

# Allow GPU memory growth instead of allocating all at once
# Recommended: true (prevents OOM on first run)
TF_FORCE_GPU_ALLOW_GROWTH=true

# cuDNN auto-tuning (can improve performance but adds startup time)
# Recommended: 1 (enable auto-tuning)
CUDNN_BENCHMARK=1

# ============================================================================
# Service Endpoints
# ============================================================================

# LM Studio endpoint (change if running on different machine)
# Default: http://127.0.0.1:1234/v1
LM_STUDIO_URL=http://127.0.0.1:1234/v1

# FishSpeech TTS endpoint
# Default: http://127.0.0.1:8080/v1/tts
FISHSPEECH_URL=http://127.0.0.1:8080/v1/tts

# xiaozhi-server WebSocket port
# Default: 8000
XIAOZHI_WS_PORT=8000

# xiaozhi-server HTTP port (OTA, utilities)
# Default: 8003
XIAOZHI_HTTP_PORT=8003

# ============================================================================
# Model Paths
# ============================================================================

# FunASR model directory (relative to xiaozhi-server)
# Default: models/SenseVoiceSmall
FUNASR_MODEL_PATH=models/SenseVoiceSmall

# FishSpeech model directory
# Default: /root/.cache/funasr/models (downloaded on first run)
FISHSPEECH_MODEL_PATH=/root/.cache/funasr/models

# ============================================================================
# Performance Tuning
# ============================================================================

# Number of ASR workers (FunASR concurrency)
# Recommended: 1-2 for 16GB VRAM (too many = OOM)
ASR_WORKERS=1

# LM Studio max concurrent requests
# Recommended: 1-2 for 13B model
LLM_MAX_CONCURRENT=1

# TTS batch size for streaming
# Recommended: 1 (lowest latency) or 4 (higher throughput)
TTS_BATCH_SIZE=1

# ============================================================================
# Logging & Debugging
# ============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Enable verbose GPU memory tracking
# Recommended: false (for production), true (for debugging)
ENABLE_GPU_MEMORY_TRACKING=false

# Enable performance profiling
# Recommended: false (adds overhead)
ENABLE_PROFILING=false

# ============================================================================
# Optional: Advanced Configuration
# ============================================================================

# Audio buffer size (samples) - affects latency/memory tradeoff
# Recommended: 16000 (1 second at 16kHz)
AUDIO_BUFFER_SIZE=16000

# VAD threshold (0.0-1.0, higher = stricter silence detection)
# Recommended: 0.5 (balanced)
VAD_THRESHOLD=0.5

# Temperature for LLM generation (0.0-1.0, higher = more creative)
# Recommended: 0.7 (balanced)
LLM_TEMPERATURE=0.7

# ============================================================================
# Notes
# ============================================================================
# 
# To use these variables:
# 1. Copy to .env:       cp .env.example .env
# 2. Edit values as needed
# 3. Load in your shell:  export $(cat .env | xargs)
# 4. Or in Python:        from python-dotenv import load_dotenv; load_dotenv()
#
# For Docker:
# 1. Copy to docker.env:  cp .env.example docker.env
# 2. Run: docker run --env-file docker.env ...
#
